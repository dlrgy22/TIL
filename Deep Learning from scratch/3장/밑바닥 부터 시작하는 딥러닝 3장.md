

# 밑바닥부터 시작하는 딥러닝 3장



신경망 -> 가중치 매개 변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력

입력층 -> 은닉층 -> 출력층

활성화 함수 -> 입력 신호의 총합을 출력 신호로 변환하는 함수

가중치가 달린 입력 신화와 편향의 총합을 활성화 함수에 넣어 출력



퍼셉트론에서는 활성화 함수로 **계단 함수**를 사용

계단 함수

~~~~python
def step_function(x):
    return np.array(x > 0, dtype=np.int)
~~~~



![step_function](/Users/jung-ikhyo/Desktop/TIL/Deep Learning from scratch/3장/step_function.png)

0을 경계로 값이 0과 1로 나뉜다.



신경망에서는 **시그모이드 함수**가 자주 사용

~~~~python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
~~~~

![sigmoid_function](/Users/jung-ikhyo/Desktop/TIL/Deep Learning from scratch/3장/sigmoid_function.png)



미분가능한 형태를 가진다. 연속적인 실수가 흐른다.



최근에는 활성화 함수로 **ReLU함수**를 주로 이용한다.

~~~~python
def relu(x):
    return np.maximum(0, x)
~~~~



![ReLU](/Users/jung-ikhyo/Desktop/TIL/Deep Learning from scratch/3장/ReLU.png)



입력이 0보다 작으면 0을 출력 0보다 크면 입력을 그대로 출력





