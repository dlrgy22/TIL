

# 밑바닥부터 시작하는 딥러닝 3장



신경망 -> 가중치 매개 변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력

입력층 -> 은닉층 -> 출력층

활성화 함수 -> 입력 신호의 총합을 출력 신호로 변환하는 함수

가중치가 달린 입력 신화와 편향의 총합을 활성화 함수에 넣어 출력



퍼셉트론에서는 활성화 함수로 **계단 함수**를 사용

계단 함수

~~~~python
def step_function(x):
    return np.array(x > 0, dtype=np.int)
~~~~


![step_function](/Users/jung-ikhyo/Desktop/TIL/Deep Learning from scratch/3장/step_function.png)

0을 경계로 값이 0과 1로 나뉜다.



신경망에서는 **시그모이드 함수**가 자주 사용

~~~~python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
~~~~

![sigmoid_function](/Users/jung-ikhyo/Desktop/TIL/Deep Learning from scratch/3장/sigmoid_function.png)



미분가능한 형태를 가진다. 연속적인 실수가 흐른다.



최근에는 활성화 함수로 **ReLU함수**를 주로 이용한다.

~~~~python
def relu(x):
    return np.maximum(0, x)
~~~~



![ReLU](/Users/jung-ikhyo/Desktop/TIL/Deep Learning from scratch/3장/ReLU.png)



입력이 0보다 작으면 0을 출력 0보다 크면 입력을 그대로 출력



**항등함수**

입력을 그대로 출력 

출력층에서 사용되면 입력 신호가 그대로 출력 신호가 된다.



**소프트맥스 함수**
$$
y_k=  (exp⁡(a_k))/(∑_i〖exp⁡(a_i 〗))
$$
소프트맥스 함수의 총합은 1 -> 확률로 해석 가능

각 원소의 대소 관계는 변하지 않는다.



출력층의 뉴런 수는 풀려는 문제에 맞게 정해야한다.

분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적이다.

Ex) 숫자 구별 -> 10개로 구별



**배치 처리**

한번에 여러개의 데이터를 처리