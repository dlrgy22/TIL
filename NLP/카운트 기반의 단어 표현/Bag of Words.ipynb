{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "streaming-venezuela",
   "metadata": {},
   "source": [
    "단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
    "\n",
    "BoW\n",
    "- 각 단어에 고유한 정수 인덱스를 부여\n",
    "- 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만든다.\n",
    "\n",
    "각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이기 때문에\n",
    "주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤성격의 문서인지를 판단하는 작업에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "native-silly",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "[1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt = Okt()\n",
    "\n",
    "token = re.sub(\"\\.\", \"\", \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\")\n",
    "token = okt.morphs(token)\n",
    "\n",
    "word2idx = {}\n",
    "bow = []\n",
    "for voca in token:\n",
    "    if voca not in word2idx:\n",
    "        word2idx[voca] = len(word2idx)\n",
    "        bow.append(1)\n",
    "    else:\n",
    "        index = word2idx[voca]\n",
    "        bow[index] += 1\n",
    "        \n",
    "print(word2idx)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "special-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "# 단어의 빈도를 Count하여 vector로 만드는 CountVectorizer => 영어에 대해서는 손쉽게 BoW를 만들 수 있다.\n",
    "# 짧은 단어 (1 길이의 단어) 제거 (정제)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"you know I want your love. because I love you.\"]\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acceptable-consent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n",
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n",
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "# 불용어는 자연어 처리에서 별로 의미를 갖지 않는 단어들 => BoW에서 불용어 제거\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "\n",
    "#직접 stopword 지정\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)\n",
    "\n",
    "#CounterVectorizer에서 제공하는 자체 불용어 사용\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words=sw)\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
